# AI3610 Big Big Homework

## Exponential Gaussian Noise

### Basic Tasks

> 训练超参数对模型在指数高斯噪声下性能的影响

- 模型: 三层 MLP
- Epochs: 5
- 优化器: Adam (默认参数)
- Batch Size: 32, 64, 128, 256
- 指数高斯噪声的标准差: 1.0
  - 由于指数高斯噪声差异较大，我们在加噪的情况下进行了 5 次测试，取平均 Acc
- 随机数种子: 1234

#### MLP

| Batch Size | Without Noise | With Noise |
| :--------: | :-----------: | :--------: |
|     32     |     97.81     |   75.48    |
|     64     |     97.98     |   72.53    |
|    128     |     97.95     |   64.89    |
|    256     |     97.48     |   63.92    |

#### CNN

| Batch Size | Without Noise | With Noise |
| :--------: | :-----------: | :--------: |
|     32     |     98.94     |   52.84    |
|     64     |     98.51     |   47.35    |
|    128     |     98.59     |   41.52    |
|    256     |     98.25     |   40.55    |

#### 结论

- Batch Size 越大，模型抗噪声能力越差
- CNN 比 MLP 更容易受噪声影响
  - 在原数据集上表现越好，越容易受噪声干扰？

### Advanced Task

#### 优化算法的改进

- 训练时对模型参数增加指数高斯的扰动
- Batch Size: 64
- 测试时噪声 Std 为 1.0
- 5 次实验取平均值
- 此外也尝试了先预训练一个无噪声的baseline模型，然后让一个有噪声的模型在训练时输出的概率尽可能接近无噪声的模型（类似 teacher-student）；目的是让模型在有噪声扰动的情况下输出尽可能接近正常模型（即输出在一定范围内尽可能平滑）
  - 接近的标准可能是 MSE 或者 KL-Divergence
  - MSELoss 标准下的模型，测试噪声方差不超过训练噪声方差时表现较好，但是测试噪声方差很大的话性能会变差
    - 从 Loss Landscape 看 MSELoss 训练的模型并没有有效降低测试时的 Loss，只是暴力让输出概率和理想情况接近
  - KLDivLoss 标准下的模型，测试噪声方差不超过训练噪声方差时表现不是特别好，但是测试噪声方差很大的话性能相对好一些
    - 但是测试时的 Loss 确实比较平缓
  - 不过两者都不如直接在训练时加噪声然后用 CrossEntropy 直接训练

|     Model      | W/o Noise | Std = 1.0 | Std = 1.5 |
| :------------: | :-------: | :-------: | :-------: |
|    Baseline    |   97.98   |   72.53   |   30.73   |
|    Std=0.5     |   98.00   |   84.38   |   44.17   |
|    Std=1.0     |   90.21   |   85.88   |   54.18   |
| TS-MSE Std=1.0 |   93.61   |   86.43   |   41.46   |
| TS-KLD Std=1.0 |   92.00   |   85.12   |   45.60   |

- 训练时加噪有助于模型抵抗指数高斯噪声的干扰
- 即使训练时的噪声 std 比实际测试时的 std 要小，模型对噪声仍然有一定的抗性
- 训练时噪声 std 较大能让模型在测试时抵抗更大的噪声，但是相应地会牺牲正常 evaluation 的性能
  - 而且训练时噪声 std 太大甚至可能导致模型训不起来

#### 模型结构的改进

- 直接在 MLP 上加 dropout (p=0.5) 就可以了（
- 但是噪声太大的时候还是会收到影响

|    Model    | W/o Noise | Std = 1.0 | Std = 1.5 |
| :---------: | :-------: | :-------: | :-------: |
|  Baseline   |   97.98   |   72.53   |   30.73   |
| Dropout=0.5 |   97.32   |   90.13   |   49.80   |

## Simluation with MemTorch

### Basic Task

> 训练超参数对模型部署到 MemTorch 之后性能的影响

- MemTorch 配置参照了 MemTorch 教程
- Device Fault 参数参照了 MemTorch 教程
  - 略有修改，假设只有 0.1 的器件卡在最高值、0.1 的器件卡在最低值；教程原版为 0.25 和 0.15
  - 即使只有 0.1，也已经足够让模型性能断崖式下跌

#### MLP

| Batch Size | PyTorch | MemTorch | MemTorch (DF) |
| :--------: | :-----: | :------: | :-----------: |
|     32     |  97.81  |  88.49   |     14.58     |
|     64     |  97.98  |  90.54   |     15.58     |
|    128     |  97.95  |  91.04   |     18.09     |
|    256     |  97.48  |  91.44   |     15.76     |

- 同时，经过指数高斯噪声训练的模型部署到 MemTorch 上并不能缓解部署时的gap，也不能有效应对 MemTorch 模拟的 Device Fault

|  Model  | PyTorch | MemTorch | MemTorch (DF) |
| :-----: | :-----: | :------: | :-----------: |
| 1.0 Std |  90.21  |  72.17   |     18.66     |
| 0.5 Std |  98.00  |  91.06   |     16.73     |

#### CNN

| Batch Size | PyTorch | MemTorch | MemTorch (DF) |
| :--------: | :-----: | :------: | :-----------: |
|     32     |  98.94  |  90.64   |     10.65     |
|     64     |  98.51  |  89.48   |     11.42     |
|    128     |  98.59  |  88.98   |     10.90     |
|    256     |  98.25  |  87.39   |     11.03     |

#### 结论

- 模型部署到 MemTorch 之后，和理想情况下的测试性能会有 gap
- 在 MemTorch 增加了 Device Fault 之后，模型性能大幅下降
- 即使时训练时考虑了指数高斯噪声干扰的模型，也不能有效应对出现 Device Fault 的情况
  - 用指数高斯噪声建模 Device Fault 带来的干扰可能并不合适

### Advanced Task

> 提升模型性能

只考虑应对 Device Fault 的情况。

#### 优化算法的改进

思路同前。Device Fault 本质上是用来表示权重矩阵的两个 Crossbar 中，有一部分器件的阻抗变成了最大值，还有一部分器件的阻抗变成了最小值；由于 Crossbar 使用阻抗表示神经网络参数，在训练时可以近似地看做：正的参数中有一部分变成了最大值和最小值，负的参数中也有一部分变成了最大和最小值。我们可以将这种扰动融入到训练过程中。

- 训练时对模型参数增加扰动，随机将一部分权重拉到最大或最小值
- Batch Size: 64

|    Model     | MemTorch (DF) |
| :----------: | :-----------: |
| MLP Baseline |     15.58     |
| MLP DFTrain  |     51.49     |

- 并且似乎一定程度上对抑制指数高斯噪声也有用

|  Model   | W/o Noise | Std = 1.0 | Std = 1.5 |
| :------: | :-------: | :-------: | :-------: |
| Baseline |   97.98   |   72.53   |   30.73   |
| DFTrain  |   97.32   |   88.32   |   69.64   |
